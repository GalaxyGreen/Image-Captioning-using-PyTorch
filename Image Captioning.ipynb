{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11 (11).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5aef884d86914ebdba3195a5827a9b70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_9534602b6b1e433cbdca68143316b13d",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_00223db2e2e541e987108a544c016952",
              "IPY_MODEL_2f233a383b5d41dbbaf1fd220e223445"
            ]
          }
        },
        "9534602b6b1e433cbdca68143316b13d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "00223db2e2e541e987108a544c016952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_639069a23768489e9c5ba163326c0dd7",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 108857766,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 108857766,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e2e3a86dee06486591e209fa66625904"
          }
        },
        "2f233a383b5d41dbbaf1fd220e223445": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_360b3fd6399545d5a83e4b5da8d6aad3",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 104M/104M [00:12&lt;00:00, 8.90MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_ac91bfd57e3c495e8ecd6cfb5ad34d62"
          }
        },
        "639069a23768489e9c5ba163326c0dd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e2e3a86dee06486591e209fa66625904": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "360b3fd6399545d5a83e4b5da8d6aad3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "ac91bfd57e3c495e8ecd6cfb5ad34d62": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmeTloXi2axk"
      },
      "source": [
        "!pip install -q Pillow gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS4ZSBDYyvzO"
      },
      "source": [
        "### Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue06QGMF2ctl"
      },
      "source": [
        "import gdown\r\n",
        "from PIL import Image\r\n",
        "import json\r\n",
        "import os\r\n",
        "import shutil\r\n",
        "\r\n",
        "import torchvision\r\n",
        "import numpy as np\r\n",
        "import pandas as pd \r\n",
        "import random\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzv5Lx4KN094"
      },
      "source": [
        "def delete_files(arr):\r\n",
        "    for file_path in arr:\r\n",
        "        if os.path.exists(file_path):\r\n",
        "            os.remove(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB3X-7GIN4rg"
      },
      "source": [
        "def delete_folders(arr):\r\n",
        "    for folder_path in arr:\r\n",
        "        if os.path.exists(folder_path):\r\n",
        "            shutil.rmtree(folder_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igvOnBEiz601"
      },
      "source": [
        "## Data downloading, preprocessing and analyzing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0uN4CZ16dXp"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YSJtXKslz_f0",
        "outputId": "b8e63aa4-4286-4c08-9e53-5c6c31bfbef8"
      },
      "source": [
        "gdown.download(\"https://drive.google.com/u/0/uc?export=download&confirm=X2sC&id=1TlNmpLUBw7jJEXgpliy29Am1HpHl-KNJ\", \"beheaded_inception3.py\", True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'beheaded_inception3.py'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vImwmfjCKcwx"
      },
      "source": [
        "### Downloading the MS COCO Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w456kVxtj-oZ"
      },
      "source": [
        "**Images download**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOek7JLuKZkc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb2ec8fa-b53e-4a8a-c612-115e11fd97ef"
      },
      "source": [
        "%%time\r\n",
        "!wget -q http://images.cocodataset.org/zips/train2017.zip\r\n",
        "print(\"Unzipping...\")\r\n",
        "!unzip -q train2017.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unzipping...\n",
            "train2017/000000259014.jpg:  write error (disk full?).  Continue? (y/n/^C) n\n",
            "\n",
            "warning:  train2017/000000259014.jpg is probably truncated\n",
            "CPU times: user 1.56 s, sys: 315 ms, total: 1.87 s\n",
            "Wall time: 17min 3s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlOHh2DbNgOH"
      },
      "source": [
        "delete_files([\"/content/train2017.zip\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcG3j95fkG8x"
      },
      "source": [
        "**Captions download**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "duFgjN0aiPy8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "839dd6f9-b817-49c4-c02e-59fcfe34f6cd"
      },
      "source": [
        "\r\n",
        "%%time\r\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\r\n",
        "print(\"Unzipping...\")\r\n",
        "!unzip -q annotations_trainval2017.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-24 22:35:11--  http://images.cocodataset.org/annotations/annotations_trainval2017.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.140.4\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.140.4|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252907541 (241M) [application/zip]\n",
            "Saving to: â€˜annotations_trainval2017.zipâ€™\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.19M  98.0MB/s    in 2.5s    \n",
            "\n",
            "2021-01-24 22:35:13 (98.0 MB/s) - â€˜annotations_trainval2017.zipâ€™ saved [252907541/252907541]\n",
            "\n",
            "Unzipping...\n",
            "CPU times: user 35.4 ms, sys: 14.3 ms, total: 49.7 ms\n",
            "Wall time: 9.86 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJPFgkj8OM3v"
      },
      "source": [
        "delete_files([\r\n",
        "    \"/content/annotations_trainval2017.zip\",\r\n",
        "    \"/content/annotations/instances_train2017.json\",\r\n",
        "    \"/content/annotations/instances_val2017.json\",\r\n",
        "    \"/content/annotations/person_keypoints_train2017.json\",\r\n",
        "    \"/content/annotations/person_keypoints_val2017.json\",\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYbugOlGr6Ep"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl2ato4p0qmS"
      },
      "source": [
        "data = json.load(open(\"/content/annotations/captions_train2017.json\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEyJVWIqYvqA"
      },
      "source": [
        "def data_table(keys):\r\n",
        "    dataframes = []\r\n",
        "\r\n",
        "    for key in keys:\r\n",
        "\r\n",
        "        if key == img_key:\r\n",
        "            key_value = \"file_name\"\r\n",
        "            key_id = \"id\"\r\n",
        "        elif key == caption_key:\r\n",
        "            key_value = \"caption\"\r\n",
        "            key_id = \"image_id\"\r\n",
        "\r\n",
        "        data_dict = {}\r\n",
        "        data_dict[key_value] = []\r\n",
        "        data_dict[\"id\"] = []\r\n",
        "\r\n",
        "        for item in data[key]:\r\n",
        "            data_dict[key_value].append(item[key_value])\r\n",
        "            data_dict[\"id\"].append(item[key_id])\r\n",
        "\r\n",
        "        dataframes.append(pd.DataFrame(data=data_dict))\r\n",
        "\r\n",
        "    return pd.merge(dataframes[0], dataframes[1], on=(\"id\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLlGNJuZy3vO"
      },
      "source": [
        "**Get rid of redundant data columns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sn8UojZNehyp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "084794d3-8915-4065-d48d-980221e23178"
      },
      "source": [
        "%%time\r\n",
        "img_key = \"images\"\r\n",
        "caption_key = \"annotations\"\r\n",
        "\r\n",
        "df = data_table((img_key, caption_key))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 591 ms, sys: 23.3 ms, total: 614 ms\n",
            "Wall time: 702 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbebj9GggSSQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "outputId": "9b1d2824-659d-43df-ea22-8d6d4691d58d"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>file_name</th>\n",
              "      <th>id</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000000391895.jpg</td>\n",
              "      <td>391895</td>\n",
              "      <td>A man with a red helmet on a small moped on a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000000391895.jpg</td>\n",
              "      <td>391895</td>\n",
              "      <td>Man riding a motor bike on a dirt road on the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000000391895.jpg</td>\n",
              "      <td>391895</td>\n",
              "      <td>A man riding on the back of a motorcycle.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000000391895.jpg</td>\n",
              "      <td>391895</td>\n",
              "      <td>A dirt path with a young person on a motor bik...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>000000391895.jpg</td>\n",
              "      <td>391895</td>\n",
              "      <td>A man in a red shirt and a red hat is on a mot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591748</th>\n",
              "      <td>000000475546.jpg</td>\n",
              "      <td>475546</td>\n",
              "      <td>The patrons enjoy their beverages at the bar.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591749</th>\n",
              "      <td>000000475546.jpg</td>\n",
              "      <td>475546</td>\n",
              "      <td>People having a drink in a basement bar.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591750</th>\n",
              "      <td>000000475546.jpg</td>\n",
              "      <td>475546</td>\n",
              "      <td>A group of friends enjoys a drink while sittin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591751</th>\n",
              "      <td>000000475546.jpg</td>\n",
              "      <td>475546</td>\n",
              "      <td>Group of people drinking wine at a public loca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>591752</th>\n",
              "      <td>000000475546.jpg</td>\n",
              "      <td>475546</td>\n",
              "      <td>Three women and a man are sitting at a bar</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>591753 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               file_name  ...                                            caption\n",
              "0       000000391895.jpg  ...  A man with a red helmet on a small moped on a ...\n",
              "1       000000391895.jpg  ...  Man riding a motor bike on a dirt road on the ...\n",
              "2       000000391895.jpg  ...          A man riding on the back of a motorcycle.\n",
              "3       000000391895.jpg  ...  A dirt path with a young person on a motor bik...\n",
              "4       000000391895.jpg  ...  A man in a red shirt and a red hat is on a mot...\n",
              "...                  ...  ...                                                ...\n",
              "591748  000000475546.jpg  ...      The patrons enjoy their beverages at the bar.\n",
              "591749  000000475546.jpg  ...           People having a drink in a basement bar.\n",
              "591750  000000475546.jpg  ...  A group of friends enjoys a drink while sittin...\n",
              "591751  000000475546.jpg  ...  Group of people drinking wine at a public loca...\n",
              "591752  000000475546.jpg  ...         Three women and a man are sitting at a bar\n",
              "\n",
              "[591753 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv5mkP0yhbO8"
      },
      "source": [
        "\r\n",
        "df.drop(columns=\"id\", inplace=True)\r\n",
        "df.rename(columns={\"file_name\": \"image\"}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sExO26-jh6cU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 254
        },
        "outputId": "b054e665-9c6a-451a-bdf5-2c15562f473d"
      },
      "source": [
        "df.head(7)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>000000391895.jpg</td>\n",
              "      <td>A man with a red helmet on a small moped on a ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000000391895.jpg</td>\n",
              "      <td>Man riding a motor bike on a dirt road on the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>000000391895.jpg</td>\n",
              "      <td>A man riding on the back of a motorcycle.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000000391895.jpg</td>\n",
              "      <td>A dirt path with a young person on a motor bik...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>000000391895.jpg</td>\n",
              "      <td>A man in a red shirt and a red hat is on a mot...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>000000522418.jpg</td>\n",
              "      <td>A woman wearing a net on her head cutting a ca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>000000522418.jpg</td>\n",
              "      <td>A woman cutting a large white sheet cake.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              image                                            caption\n",
              "0  000000391895.jpg  A man with a red helmet on a small moped on a ...\n",
              "1  000000391895.jpg  Man riding a motor bike on a dirt road on the ...\n",
              "2  000000391895.jpg          A man riding on the back of a motorcycle.\n",
              "3  000000391895.jpg  A dirt path with a young person on a motor bik...\n",
              "4  000000391895.jpg  A man in a red shirt and a red hat is on a mot...\n",
              "5  000000522418.jpg  A woman wearing a net on her head cutting a ca...\n",
              "6  000000522418.jpg          A woman cutting a large white sheet cake."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8luBEi02p-mg"
      },
      "source": [
        "-- Empty table data check --"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X7xSxI7Nh8G3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "617193e5-f2be-4960-9e7d-5817d26b9e78"
      },
      "source": [
        "df.isna().any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "image      False\n",
              "caption    False\n",
              "dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HOkIWEJzOeT"
      },
      "source": [
        "## Creating datasets, iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SozVMqs62jhb"
      },
      "source": [
        "IMAGE_COL_LABEL = df.columns[0]\r\n",
        "CAPTION_COL_LABEL = df.columns[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OPHdzGms4k-"
      },
      "source": [
        "import string\r\n",
        "\r\n",
        "class TextDataset(Dataset):\r\n",
        "    \r\n",
        "    BOS = \"<BOS>\"\r\n",
        "    EOS = \"<EOS>\"\r\n",
        "    PAD = \"<PAD>\"\r\n",
        "\r\n",
        "    def __init__(self, text_data: pd.Series, min_word_freq=None, max_word_freq=None, max_sentence_len=20, lower=True):\r\n",
        "\r\n",
        "        if min_word_freq is None:\r\n",
        "            self.min_word_freq = -np.inf\r\n",
        "        else:\r\n",
        "            self.min_word_freq = min_word_freq\r\n",
        "\r\n",
        "        if max_word_freq is None:\r\n",
        "            self.max_word_freq = np.inf\r\n",
        "        else:\r\n",
        "            self.max_word_freq = max_word_freq\r\n",
        "\r\n",
        "\r\n",
        "        self.max_sentence_len = max_sentence_len\r\n",
        "        self.lower = lower\r\n",
        "\r\n",
        "        self.tokenizer = nltk.WordPunctTokenizer()\r\n",
        "        self.additional_tokens = (self.BOS, self.EOS, self.PAD)\r\n",
        "        self.token2idx = {self.BOS: 1, self.EOS: 2, self.PAD: 3}\r\n",
        "        self.vocab = set([self.BOS, self.EOS, self.PAD])\r\n",
        "\r\n",
        "        for row in text_data:\r\n",
        "            self.vocab.update([token for token in self.__tokenize(row) if token not in self.additional_tokens])\r\n",
        "  \r\n",
        "        self.__remove_rare_words(text_data)\r\n",
        "        self.token2idx.update({token:num + len(self.additional_tokens) for num, token in enumerate(self.vocab) if token not in self.additional_tokens})\r\n",
        "        self.idx2token = {num: token for token, num in self.token2idx.items()}\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.token2idx.keys())\r\n",
        "\r\n",
        "    def __getitem__(self, text):            \r\n",
        "        tokens = self.__tokenize(text)\r\n",
        "        text_ids = [self.token2idx[token] for token in tokens]        \r\n",
        "        return text_ids\r\n",
        "\r\n",
        "    def __tokenize(self, row):\r\n",
        "        cut_edge = self.max_sentence_len - 2\r\n",
        "\r\n",
        "        row = row.lower() if self.lower else row\r\n",
        "        tokens = self.tokenizer.tokenize(row)\r\n",
        "        tokens = [token for token in tokens if token not in string.punctuation]\r\n",
        "        tokens = [token for token in tokens if token in self.vocab]\r\n",
        "        tokens = tokens[:cut_edge]\r\n",
        "        tokens = [self.BOS] + tokens + [self.EOS]\r\n",
        "        tokens = tokens + (self.max_sentence_len - len(tokens)) * [self.PAD]\r\n",
        "        return tokens\r\n",
        "\r\n",
        "    def __remove_rare_words(self, text: pd.Series):\r\n",
        "        counted_words = text.str.lower().str.split().explode().value_counts()\r\n",
        "\r\n",
        "        remove_list =  [key for key, val in dict(counted_words).items() \r\n",
        "                        if (val < self.min_word_freq or val > self.max_word_freq) and (key not in self.additional_tokens)]\r\n",
        "\r\n",
        "        for key in remove_list:\r\n",
        "            self.vocab.discard(key)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lEbw5bvV_Y8i"
      },
      "source": [
        "tokenizer = nltk.WordPunctTokenizer()\r\n",
        "max_sentence_len = 20\r\n",
        "\r\n",
        "def tokenize(row):\r\n",
        "    cut_edge = max_sentence_len - 2\r\n",
        "\r\n",
        "    row = row.lower()\r\n",
        "    tokens = tokenizer.tokenize(row)\r\n",
        "    tokens = [token for token in tokens if token not in string.punctuation]\r\n",
        "    tokens = tokens[:cut_edge]\r\n",
        "    tokens = [1] + tokens + [2]\r\n",
        "    tokens = tokens + (max_sentence_len - len(tokens)) * [3]\r\n",
        "    return tokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqNQuZIKySFc"
      },
      "source": [
        "### Creating text dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qiiA6F5vJG8Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "085d5e51-67b0-4ed1-eb94-8b368f22b05a"
      },
      "source": [
        "%%time\r\n",
        "text_data = df.iloc[:, 1]\r\n",
        "text_dataset = TextDataset(text_data=text_data, min_word_freq=5, max_sentence_len=20, lower=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 9.37 s, sys: 186 ms, total: 9.56 s\n",
            "Wall time: 9.67 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzIvYnxnyLqV"
      },
      "source": [
        "### Applying tokenization for data captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Auu1UEJo3_3V"
      },
      "source": [
        "df[CAPTION_COL_LABEL] = df[CAPTION_COL_LABEL].apply(lambda x: text_dataset[x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpOOhVu1yDvA"
      },
      "source": [
        "### BatchIterator class for creating data iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkV5xezMnaGi"
      },
      "source": [
        "class BatchIterator:\r\n",
        "    def __init__(self, dataframe, unique_images, image_col_label, caption_col_label, batch_size, image_transformer, main_img_path, shuffle=False):\r\n",
        "        self.dataframe = dataframe\r\n",
        "        self.unique_images = unique_images\r\n",
        "        self.image_col = image_col_label\r\n",
        "        self.caption_col = caption_col_label\r\n",
        "        self.image_transformer = image_transformer\r\n",
        "        self.main_img_path = main_img_path\r\n",
        "\r\n",
        "        self.num_samples = len(unique_images)\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.batches_count =  self.num_samples // self.batch_size\r\n",
        "        self.shuffle = shuffle\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.batches_count\r\n",
        "\r\n",
        "    def __iter__(self):\r\n",
        "        indices = np.arange(self.num_samples)\r\n",
        "\r\n",
        "        if self.shuffle:\r\n",
        "            np.random.shuffle(indices)\r\n",
        "\r\n",
        "        for start in range(0, self.num_samples, self.batch_size):\r\n",
        "            end = min(start + self.batch_size, self.num_samples)\r\n",
        "\r\n",
        "            batch_indices = indices[start:end]\r\n",
        "            batch_images = []\r\n",
        "            batch_captions = []\r\n",
        "\r\n",
        "            for idx in batch_indices:\r\n",
        "                image_name = self.dataframe[self.image_col][idx]\r\n",
        "\r\n",
        "                image = self.__get_image_matrix(image_name)                \r\n",
        "                caption = self.__get_caption(image_name)\r\n",
        "\r\n",
        "                batch_images.append(image)\r\n",
        "                batch_captions.append(caption)\r\n",
        "\r\n",
        "            yield {\r\n",
        "                \"images\": batch_images,\r\n",
        "                \"captions\": torch.tensor(batch_captions)\r\n",
        "            }\r\n",
        "\r\n",
        "    def __get_image_matrix(self, image_name):\r\n",
        "        image = Image.open(os.path.join(self.main_img_path, image_name))\r\n",
        "        return self.image_transformer(image)\r\n",
        "\r\n",
        "    def __get_caption(self, image_name):\r\n",
        "        all_captions = np.array(\r\n",
        "                        self.dataframe[self.dataframe[self.image_col] == image_name][self.caption_col]\r\n",
        "                      )\r\n",
        "        return random.choice(all_captions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI31TXLTt5nN"
      },
      "source": [
        "def split_data(dataframe, ratios):\r\n",
        "    data_len = dataframe.shape[0]\r\n",
        "\r\n",
        "    lengths = [int(data_len * ratio) for ratio in ratios]\r\n",
        "    if np.sum(lengths) != data_len:\r\n",
        "        lengths[-1] = data_len - np.sum(lengths[:-1])\r\n",
        "    \r\n",
        "    split_indices = [np.sum(lengths[:i+1]) for i in range(len(lengths))]\r\n",
        "    return np.split(df, split_indices)[:len(lengths)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR7Jmne6x9-2"
      },
      "source": [
        "### Spliting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeRZwaiAhdCg"
      },
      "source": [
        "train_data, valid_data, test_data = split_data(df, [0.8, 0.1, 0.1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpnhls2Qt7qw"
      },
      "source": [
        "### Creating data iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I45lfuF2PMM"
      },
      "source": [
        "image_transformer = torchvision.transforms.Compose([\r\n",
        "                          torchvision.transforms.ToTensor(),\r\n",
        "                          torchvision.transforms.Resize((299, 299)),\r\n",
        "                          torchvision.transforms.Normalize([0.485, 0.456, 0.406],\r\n",
        "                                                           [0.229, 0.224, 0.225])\r\n",
        "                    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5PhGf-20hH0"
      },
      "source": [
        "BATCH_SIZE = 32\r\n",
        "unique_images = np.unique(df[IMAGE_COL_LABEL])\r\n",
        "main_img_path = \"/content/train2017\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu1eyytEqymx"
      },
      "source": [
        "train_iterator = BatchIterator(train_data, unique_images, IMAGE_COL_LABEL, CAPTION_COL_LABEL, BATCH_SIZE, image_transformer, main_img_path)\r\n",
        "valid_iterator = BatchIterator(valid_data, unique_images, IMAGE_COL_LABEL, CAPTION_COL_LABEL, BATCH_SIZE, image_transformer, main_img_path)\r\n",
        "test_iterator = BatchIterator(test_data, unique_images, IMAGE_COL_LABEL, CAPTION_COL_LABEL, BATCH_SIZE, image_transformer, main_img_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu69FBZQYOzL"
      },
      "source": [
        "# Main Part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1QT-hjLzsWD"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fo19dru1bh1"
      },
      "source": [
        "**ENCODER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypRxw45XJqwC"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, cnn_model, embedding_size, cnn_feature_size=2048):\r\n",
        "        super().__init__()\r\n",
        "        self.cnn_model = cnn_model\r\n",
        "        self.fc = nn.Linear(cnn_feature_size, embedding_size) \r\n",
        "\r\n",
        "    def forward(self, images):\r\n",
        "        _, features, logits = self.cnn_model(images)\r\n",
        "        features = self.fc(features)\r\n",
        "        return features "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UstfHFbjkcwt"
      },
      "source": [
        "**DECODER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azMNeYk2jXl7"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.3, num_layers=2, bidirectional=True, cnn_feature_size=2048):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        num_directions = 2 if bidirectional else 1\r\n",
        "        assert hidden_dim % num_directions == 0\r\n",
        "        rnn_hidden_dim = hidden_dim // 2\r\n",
        "\r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\r\n",
        "        self.rnn = nn.LSTM(embedding_dim, rnn_hidden_dim, num_layers=num_layers,\r\n",
        "                           dropout=dropout, bidirectional=bidirectional, batch_first=True)\r\n",
        "\r\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\r\n",
        "\r\n",
        "    def forward(self, image_vectors, captions):\r\n",
        "        embedded = self.embedding_layer(captions)\r\n",
        "        \r\n",
        "        print(\"Embedded shape: \", embedded.shape)\r\n",
        "        print(\"image_vectors shape: \", image_vectors.shape)\r\n",
        "\r\n",
        "        concated_data = torch.cat((image_vectors.unsqueeze(1), embedded), dim=1)\r\n",
        "        rnn_output, hidden = self.rnn(concated_data)\r\n",
        "        logits = self.fc(rnn_output)\r\n",
        "        return logits, hidden "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DEAQxCC-RpmP"
      },
      "source": [
        "        # captions = captions[:, :-1]\r\n",
        "        # embed = self.embedding_layer(captions)\r\n",
        "        # embed = torch.cat((features.unsqueeze(1), embed), dim = 1)\r\n",
        "        # lstm_outputs, _ = self.lstm(embed)\r\n",
        "        # out = self.linear(lstm_outputs)\r\n",
        "        \r\n",
        "        # return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyeX5kVtKXd9"
      },
      "source": [
        "**SEQ-2-SEQ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oGjQUedEgYM"
      },
      "source": [
        "class CaptionNet(nn.Module):\r\n",
        "    def __init__(self, encoder, decoder, device, teacher_forcing_ratio = 0.5):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        self.device = device\r\n",
        "        self.teacher_forcing_ratio = teacher_forcing_ratio\r\n",
        "\r\n",
        "    def forward(self, images, captions):\r\n",
        "        features = self.encoder(images)\r\n",
        "\r\n",
        "        batch_size = captions.shape[0]\r\n",
        "        max_len = captions.shape[1]\r\n",
        "        \r\n",
        "        #tensor to store decoder outputs\r\n",
        "        outputs = torch.zeros(max_len, batch_size, self.decoder.vocab_size).to(self.device)\r\n",
        "        \r\n",
        "        for idx in range(1, max_len):\r\n",
        "            output = self.decoder(features, captions)\r\n",
        "            \r\n",
        "            outputs[idx] = output\r\n",
        "            # teacher_force = random.random() < teacher_forcing_ratio\r\n",
        "            # top word or ground truth\r\n",
        "            # input = (captions[idx] if teacher_force else output.max(1)[1])\r\n",
        "        \r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eJc5H5dRt6W"
      },
      "source": [
        "    # def sample(self, inputs, states=None, max_len=20):\r\n",
        "    #     \" accepts pre-processed  image tensor (inputs) and returns predicted sentence (list of tensor ids of length max_len) \"\r\n",
        "    #     output_sentence = []\r\n",
        "    #     for i in range(max_len):\r\n",
        "    #         lstm_outputs, states = self.lstm(inputs, states)\r\n",
        "    #         lstm_outputs = lstm_outputs.squeeze(1)\r\n",
        "    #         out = self.linear(lstm_outputs)\r\n",
        "    #         last_pick = out.max(1)[1]\r\n",
        "    #         output_sentence.append(last_pick.item())\r\n",
        "    #         inputs = self.embedding_layer(last_pick).unsqueeze(1)\r\n",
        "        \r\n",
        "    #     return output_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fX1k_gJdQw9L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85ce6588-342f-4331-c20e-8b620df47cab"
      },
      "source": [
        "#@title Hyperparameters { run: \"auto\" }\r\n",
        "\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(\"Device: \", device)\r\n",
        "\r\n",
        "VOCAB_SIZE = len(text_dataset.vocab)\r\n",
        "EMBEDDING_DIM = 250 #@param {type:\"slider\", min:100, max:1000, step:50}\r\n",
        "HIDDEN_DIM = 128 #@param [\"128\", \"256\", \"512\", \"1024\", \"2048\"] {type:\"raw\"}\r\n",
        "DROPOUT = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.1}\r\n",
        "NUM_LAYERS = 2 #@param {type:\"slider\", min:1, max:10, step:1}\r\n",
        "BIDIRECTIONAL = True #@param {type:\"boolean\"}\r\n",
        "TEACHER_FORCE_RATIO = 0.3 #@param {type:\"slider\", min:0, max:1, step:0.1}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xERVY_VfcP_A"
      },
      "source": [
        "Inception model loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZSuJel6cKs7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "5aef884d86914ebdba3195a5827a9b70",
            "9534602b6b1e433cbdca68143316b13d",
            "00223db2e2e541e987108a544c016952",
            "2f233a383b5d41dbbaf1fd220e223445",
            "639069a23768489e9c5ba163326c0dd7",
            "e2e3a86dee06486591e209fa66625904",
            "360b3fd6399545d5a83e4b5da8d6aad3",
            "ac91bfd57e3c495e8ecd6cfb5ad34d62"
          ]
        },
        "outputId": "844c4629-f8ae-45c1-bfd7-84460495b873"
      },
      "source": [
        "%%time\r\n",
        "from beheaded_inception3 import beheaded_inception_v3\r\n",
        "inception = beheaded_inception_v3().train(False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5aef884d86914ebdba3195a5827a9b70",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=108857766.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "CPU times: user 3min 32s, sys: 999 ms, total: 3min 33s\n",
            "Wall time: 3min 34s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Va_DoRRsAZY4"
      },
      "source": [
        "def deny_param_train(model):\r\n",
        "    for param in model.parameters():\r\n",
        "        param.requires_grad = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qBKjOuMAoni"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return np.sum([param.numel() for param in model.parameters() if param.requires_grad])\r\n",
        "# print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "armn4FjEYrAP"
      },
      "source": [
        "encoder = Encoder(inception, EMBEDDING_DIM).to(device)\r\n",
        "decoder = Decoder(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, DROPOUT, NUM_LAYERS, BIDIRECTIONAL).to(device)\r\n",
        "# decoder = Decoder(HIDDEN_DIM, VOCAB_SIZE, DROPOUT, NUM_LAYERS, BIDIRECTIONAL).to(device)\r\n",
        "captionNet = CaptionNet(encoder, decoder, device, TEACHER_FORCE_RATIO).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQ2bLqhKAllD"
      },
      "source": [
        "deny_param_train(encoder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRt_2KeOg1Z7"
      },
      "source": [
        "optimizer = torch.optim.Adam(captionNet.parameters())\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=3).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWpsLmnogEwD"
      },
      "source": [
        "#@title {run: \"auto\" }\r\n",
        "\r\n",
        "EPOCHS = 6 #@param {type:\"slider\", min:1, max:20, step:1}\r\n",
        "CLIP = 2 #@param {type:\"slider\", min:1, max:10, step:1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxE6vTE4dxXi"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4zt3rsFb9e9"
      },
      "source": [
        "def iterate_model(mode, model, iterator, optimizer, criterion, device, clip):\r\n",
        "    if mode == \"train\":\r\n",
        "        model.train()\r\n",
        "    elif mode == \"valid\":\r\n",
        "        model.eval()\r\n",
        "    else:\r\n",
        "      raise ValueError(\"Invalid mode, must be 'train' or 'valid'\")\r\n",
        "\r\n",
        "    epoch_loss = 0\r\n",
        "    history = []\r\n",
        "    for iteration, batch in tqdm(enumerate(iterator), total=len(iterator)):\r\n",
        "\r\n",
        "        images = torch.stack(batch[\"images\"]).to(device)\r\n",
        "        captions = batch[\"captions\"].to(device)\r\n",
        "\r\n",
        "        output = model(images, captions)\r\n",
        "        loss = criterion(output, captions)\r\n",
        "        \r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "        optimizer.step()\r\n",
        "\r\n",
        "        epoch_loss += loss.item()\r\n",
        "\r\n",
        "    epoch_loss /= len(iterator)\r\n",
        "\r\n",
        "    return epoch_loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lsZP2t34rdx9"
      },
      "source": [
        "MIN_LOSS = np.inf\r\n",
        "CUR_PATIENCE = 0\r\n",
        "PATIENCE = 2\r\n",
        "SAVE_EPOCH = 2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABuLojdttF37"
      },
      "source": [
        "for epoch in range(EPOCHS):\r\n",
        "        \r\n",
        "    train_loss = iterate_model(\"train\", captionNet, train_iterator, optimizer, criterion, device, CLIP)\r\n",
        "    valid_loss = iterate_model(\"valid\", captionNet, valid_iterator, criterion)\r\n",
        "    \r\n",
        "    if valid_loss < MIN_LOSS:\r\n",
        "        MIN_LOSS = valid_loss\r\n",
        "        best_model = bert_clf.state_dict()\r\n",
        "    else:\r\n",
        "        CUR_PATIENCE += 1\r\n",
        "        if CUR_PATIENCE == PATIENCE:\r\n",
        "            CUR_PATIENCE = 0\r\n",
        "            break\r\n",
        "\r\n",
        "    if (epoch + 1) % SAVE_EPOCH == 0:\r\n",
        "        torch.save(best_model, 'best-model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBjZv2crBi7i"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}