{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled11_(11)_(8) (3).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "deb9c8652b8c40ec90865028ce30ea6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_7d6bea0a81f44205bf8ed99cedfebdf1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_00b3019b8b4448cb9a6ce1f17d516835",
              "IPY_MODEL_0e5e0940f61e47e1a5c1267790010fe0"
            ]
          }
        },
        "7d6bea0a81f44205bf8ed99cedfebdf1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "00b3019b8b4448cb9a6ce1f17d516835": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_b20b11fe3e8949e58d6ed996447782e9",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 108857766,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 108857766,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_18e57034e50745c58ece39c4611fa160"
          }
        },
        "0e5e0940f61e47e1a5c1267790010fe0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_37bf3ddae5d347eb8ab8f67d00feec1d",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 104M/104M [00:01&lt;00:00, 91.9MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_243662727a104bf78f0b63cc437a49eb"
          }
        },
        "b20b11fe3e8949e58d6ed996447782e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "18e57034e50745c58ece39c4611fa160": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "37bf3ddae5d347eb8ab8f67d00feec1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "243662727a104bf78f0b63cc437a49eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmeTloXi2axk"
      },
      "source": [
        "!pip install -q Pillow gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RS4ZSBDYyvzO"
      },
      "source": [
        "### Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ue06QGMF2ctl"
      },
      "source": [
        "import gdown\r\n",
        "from PIL import Image\r\n",
        "from PIL import ImageFile\r\n",
        "ImageFile.LOAD_TRUNCATED_IMAGES = True\r\n",
        "\r\n",
        "import json\r\n",
        "import os\r\n",
        "import shutil\r\n",
        "\r\n",
        "import torchvision\r\n",
        "import numpy as np\r\n",
        "import pandas as pd \r\n",
        "import random\r\n",
        "\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "from torch.utils.data import Dataset\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "import nltk"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzv5Lx4KN094"
      },
      "source": [
        "def delete_files(arr):\r\n",
        "    for file_path in arr:\r\n",
        "        if os.path.exists(file_path):\r\n",
        "            os.remove(file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB3X-7GIN4rg"
      },
      "source": [
        "\r\n",
        "def delete_folders(arr):\r\n",
        "    for folder_path in arr:\r\n",
        "        if os.path.exists(folder_path):\r\n",
        "            shutil.rmtree(folder_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igvOnBEiz601"
      },
      "source": [
        "## Data downloading, preprocessing and analyzing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0uN4CZ16dXp"
      },
      "source": [
        "Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YSJtXKslz_f0",
        "outputId": "a6965a50-5358-4d5d-9aae-8441dbb02f11"
      },
      "source": [
        "gdown.download(\"https://drive.google.com/u/0/uc?export=download&confirm=X2sC&id=1TlNmpLUBw7jJEXgpliy29Am1HpHl-KNJ\", \"beheaded_inception3.py\", True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'beheaded_inception3.py'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vImwmfjCKcwx"
      },
      "source": [
        "### Downloading the MS COCO Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w456kVxtj-oZ"
      },
      "source": [
        "**Images download**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kOek7JLuKZkc",
        "outputId": "3f236a3a-857b-4ff7-a8a0-fa78117f7c30"
      },
      "source": [
        "%%time\r\n",
        "# !wget -q http://images.cocodataset.org/zips/train2017.zip\r\n",
        "!wget -q http://images.cocodataset.org/zips/train2014.zip\r\n",
        "print(\"Unzipping...\")\r\n",
        "# !unzip -q train2017.zip\r\n",
        "!unzip -q train2014.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unzipping...\n",
            "CPU times: user 1.01 s, sys: 210 ms, total: 1.22 s\n",
            "Wall time: 11min 30s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlOHh2DbNgOH"
      },
      "source": [
        "delete_files([\"/content/train2014.zip\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcG3j95fkG8x"
      },
      "source": [
        "**Captions download**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "duFgjN0aiPy8",
        "outputId": "a39b9fb7-d253-4a1b-e334-01c988fa4f31"
      },
      "source": [
        "\r\n",
        "%%time\r\n",
        "# !wget http://images.cocodataset.org/annotations/annotations_trainval2017.zip\r\n",
        "!wget http://images.cocodataset.org/annotations/annotations_trainval2014.zip\r\n",
        "print(\"Unzipping...\")\r\n",
        "# !unzip -q annotations_trainval2017.zip\r\n",
        "!unzip -q annotations_trainval2014.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-31 08:24:01--  http://images.cocodataset.org/annotations/annotations_trainval2014.zip\n",
            "Resolving images.cocodataset.org (images.cocodataset.org)... 52.216.108.11\n",
            "Connecting to images.cocodataset.org (images.cocodataset.org)|52.216.108.11|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 252872794 (241M) [application/zip]\n",
            "Saving to: â€˜annotations_trainval2014.zipâ€™\n",
            "\n",
            "annotations_trainva 100%[===================>] 241.16M  98.7MB/s    in 2.4s    \n",
            "\n",
            "2021-01-31 08:24:03 (98.7 MB/s) - â€˜annotations_trainval2014.zipâ€™ saved [252872794/252872794]\n",
            "\n",
            "Unzipping...\n",
            "CPU times: user 30.8 ms, sys: 21.5 ms, total: 52.3 ms\n",
            "Wall time: 9.82 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aJPFgkj8OM3v"
      },
      "source": [
        "delete_files([\r\n",
        "    \"/content/annotations/instances_train2014.json\",\r\n",
        "    \"/content/annotations/instances_val2014.json\",\r\n",
        "    \"/content/annotations/person_keypoints_train2014.json\",\r\n",
        "    \"/content/annotations/person_keypoints_val2014.json\",\r\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYbugOlGr6Ep"
      },
      "source": [
        "### Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yl2ato4p0qmS"
      },
      "source": [
        "data = json.load(open(\"/content/annotations/captions_train2014.json\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEyJVWIqYvqA"
      },
      "source": [
        "def data_table(keys):\r\n",
        "    dataframes = []\r\n",
        "\r\n",
        "    for key in keys:\r\n",
        "\r\n",
        "        if key == img_key:\r\n",
        "            key_value = \"file_name\"\r\n",
        "            key_id = \"id\"\r\n",
        "        elif key == caption_key:\r\n",
        "            key_value = \"caption\"\r\n",
        "            key_id = \"image_id\"\r\n",
        "\r\n",
        "        data_dict = {}\r\n",
        "        data_dict[key_value] = []\r\n",
        "        data_dict[\"id\"] = []\r\n",
        "\r\n",
        "        for item in data[key]:\r\n",
        "            data_dict[key_value].append(item[key_value])\r\n",
        "            data_dict[\"id\"].append(item[key_id])\r\n",
        "\r\n",
        "        dataframes.append(pd.DataFrame(data=data_dict))\r\n",
        "\r\n",
        "    return pd.merge(dataframes[0], dataframes[1], on=(\"id\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLlGNJuZy3vO"
      },
      "source": [
        "**Get rid of redundant data columns**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sn8UojZNehyp",
        "outputId": "77b2b6c6-bcb4-4a08-f31e-c9a695a2657b"
      },
      "source": [
        "%%time\r\n",
        "img_key = \"images\"\r\n",
        "caption_key = \"annotations\"\r\n",
        "\r\n",
        "df = data_table((img_key, caption_key))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 432 ms, sys: 19.4 ms, total: 452 ms\n",
            "Wall time: 502 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sv5mkP0yhbO8"
      },
      "source": [
        "df.drop(columns=\"id\", inplace=True)\r\n",
        "df.rename(columns={\"file_name\": \"image\"}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "sExO26-jh6cU",
        "outputId": "97665457-c8c4-4507-f684-c692cc75ed03"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>COCO_train2014_000000057870.jpg</td>\n",
              "      <td>A restaurant has modern wooden tables and chairs.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>COCO_train2014_000000057870.jpg</td>\n",
              "      <td>A long restaurant table with rattan rounded ba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>COCO_train2014_000000057870.jpg</td>\n",
              "      <td>a long table with a plant on top of it surroun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>COCO_train2014_000000057870.jpg</td>\n",
              "      <td>A long table with a flower arrangement in the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>COCO_train2014_000000057870.jpg</td>\n",
              "      <td>A table is adorned with wooden chairs with blu...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                             image                                            caption\n",
              "0  COCO_train2014_000000057870.jpg  A restaurant has modern wooden tables and chairs.\n",
              "1  COCO_train2014_000000057870.jpg  A long restaurant table with rattan rounded ba...\n",
              "2  COCO_train2014_000000057870.jpg  a long table with a plant on top of it surroun...\n",
              "3  COCO_train2014_000000057870.jpg  A long table with a flower arrangement in the ...\n",
              "4  COCO_train2014_000000057870.jpg  A table is adorned with wooden chairs with blu..."
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HpAd2b0r1AvJ"
      },
      "source": [
        "df = df[df['image'].isin(os.listdir(\"/content/train2014\"))]\r\n",
        "df.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 402
        },
        "id": "r3srKXkW1oMX",
        "outputId": "7ebdbdaf-d77e-45ec-da9e-39770bf0b1e2"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>image</th>\n",
              "      <th>caption</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>COCO_train2014_000000057870.jpg</td>\n",
              "      <td>A restaurant has modern wooden tables and chairs.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>COCO_train2014_000000057870.jpg</td>\n",
              "      <td>A long restaurant table with rattan rounded ba...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>COCO_train2014_000000057870.jpg</td>\n",
              "      <td>a long table with a plant on top of it surroun...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>COCO_train2014_000000057870.jpg</td>\n",
              "      <td>A long table with a flower arrangement in the ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>COCO_train2014_000000057870.jpg</td>\n",
              "      <td>A table is adorned with wooden chairs with blu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414108</th>\n",
              "      <td>COCO_train2014_000000475546.jpg</td>\n",
              "      <td>The patrons enjoy their beverages at the bar.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414109</th>\n",
              "      <td>COCO_train2014_000000475546.jpg</td>\n",
              "      <td>People having a drink in a basement bar.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414110</th>\n",
              "      <td>COCO_train2014_000000475546.jpg</td>\n",
              "      <td>A group of friends enjoys a drink while sittin...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414111</th>\n",
              "      <td>COCO_train2014_000000475546.jpg</td>\n",
              "      <td>Group of people drinking wine at a public loca...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>414112</th>\n",
              "      <td>COCO_train2014_000000475546.jpg</td>\n",
              "      <td>Three women and a man are sitting at a bar</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>414113 rows Ã— 2 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                  image                                            caption\n",
              "0       COCO_train2014_000000057870.jpg  A restaurant has modern wooden tables and chairs.\n",
              "1       COCO_train2014_000000057870.jpg  A long restaurant table with rattan rounded ba...\n",
              "2       COCO_train2014_000000057870.jpg  a long table with a plant on top of it surroun...\n",
              "3       COCO_train2014_000000057870.jpg  A long table with a flower arrangement in the ...\n",
              "4       COCO_train2014_000000057870.jpg  A table is adorned with wooden chairs with blu...\n",
              "...                                 ...                                                ...\n",
              "414108  COCO_train2014_000000475546.jpg      The patrons enjoy their beverages at the bar.\n",
              "414109  COCO_train2014_000000475546.jpg           People having a drink in a basement bar.\n",
              "414110  COCO_train2014_000000475546.jpg  A group of friends enjoys a drink while sittin...\n",
              "414111  COCO_train2014_000000475546.jpg  Group of people drinking wine at a public loca...\n",
              "414112  COCO_train2014_000000475546.jpg         Three women and a man are sitting at a bar\n",
              "\n",
              "[414113 rows x 2 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8luBEi02p-mg"
      },
      "source": [
        "-- Empty table data check --"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X7xSxI7Nh8G3",
        "outputId": "fca5844c-cee2-474f-ea35-dd581cf31f85"
      },
      "source": [
        "df.isna().any()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "image      False\n",
              "caption    False\n",
              "dtype: bool"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_HOkIWEJzOeT"
      },
      "source": [
        "## Creating datasets, iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SozVMqs62jhb"
      },
      "source": [
        "IMAGE_COL_LABEL = df.columns[0]\r\n",
        "CAPTION_COL_LABEL = df.columns[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OPHdzGms4k-"
      },
      "source": [
        "import string\r\n",
        "\r\n",
        "class TextDataset(Dataset):\r\n",
        "    \r\n",
        "    BOS = \"<BOS>\"\r\n",
        "    EOS = \"<EOS>\"\r\n",
        "    PAD = \"<PAD>\"\r\n",
        "\r\n",
        "    def __init__(self, text_data: pd.Series, min_word_freq=None, max_word_freq=None, max_sentence_len=20, lower=True):\r\n",
        "\r\n",
        "        if min_word_freq is None:\r\n",
        "            self.min_word_freq = -np.inf\r\n",
        "        else:\r\n",
        "            self.min_word_freq = min_word_freq\r\n",
        "\r\n",
        "        if max_word_freq is None:\r\n",
        "            self.max_word_freq = np.inf\r\n",
        "        else:\r\n",
        "            self.max_word_freq = max_word_freq\r\n",
        "\r\n",
        "\r\n",
        "        self.max_sentence_len = max_sentence_len\r\n",
        "        self.lower = lower\r\n",
        "\r\n",
        "        self.tokenizer = nltk.WordPunctTokenizer()\r\n",
        "        self.additional_tokens = set([self.BOS, self.EOS, self.PAD])\r\n",
        "        self.token2idx = {self.BOS: 1, self.EOS: 2, self.PAD: 0}\r\n",
        "\r\n",
        "        self.vocab = set(self.append_words(text_data))\r\n",
        "        self.token2idx.update({token:num + len(self.additional_tokens) for num, token in enumerate(self.vocab) if token not in self.additional_tokens})\r\n",
        "\r\n",
        "        self.idx2token = {num: token for token, num in self.token2idx.items()}\r\n",
        "        self.vocab = self.additional_tokens.union(self.vocab)\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.vocab)\r\n",
        "\r\n",
        "    def __getitem__(self, text):            \r\n",
        "        tokens = self.__tokenize(text)\r\n",
        "        text_ids = [token for token in tokens if token in self.vocab]\r\n",
        "        text_ids = self.pad_sequence(text_ids)        \r\n",
        "        text_ids = [self.token2idx[token] for token in text_ids]\r\n",
        "        return text_ids\r\n",
        "\r\n",
        "    def __tokenize(self, row):\r\n",
        "        cut_edge = self.max_sentence_len - 2\r\n",
        "\r\n",
        "        row = row.lower() if self.lower else row\r\n",
        "        tokens = self.tokenizer.tokenize(row)\r\n",
        "        tokens = [token for token in tokens if token not in string.punctuation]\r\n",
        "        tokens = tokens[:cut_edge]\r\n",
        "        tokens = [self.BOS] + tokens + [self.EOS]\r\n",
        "        tokens = self.pad_sequence(tokens)\r\n",
        "        return tokens\r\n",
        "\r\n",
        "    def pad_sequence(self, tokens):\r\n",
        "        padded_sequence = tokens + (self.max_sentence_len - len(tokens)) * [self.PAD]\r\n",
        "        return padded_sequence\r\n",
        "\r\n",
        "    def append_words(self, text: pd.Series):\r\n",
        "        counted_words = text.str.lower().str.split().explode().value_counts()\r\n",
        "        append_list =  [key for key, val in dict(counted_words).items() \r\n",
        "                        if (val > self.min_word_freq and val < self.max_word_freq) and (key not in self.additional_tokens)]\r\n",
        "\r\n",
        "        return append_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zDWz_t8mZnTL",
        "outputId": "23b9fea7-41e1-4b4d-8cb8-67d880a228e5"
      },
      "source": [
        "%%time\r\n",
        "text_data = df.iloc[:, 1]\r\n",
        "text_dataset = TextDataset(text_data=text_data, min_word_freq=5, max_sentence_len=20, lower=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 3.06 s, sys: 143 ms, total: 3.2 s\n",
            "Wall time: 3.24 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LqNQuZIKySFc"
      },
      "source": [
        "### Creating text dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzIvYnxnyLqV"
      },
      "source": [
        "### Applying tokenization for data captions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Auu1UEJo3_3V"
      },
      "source": [
        "df[CAPTION_COL_LABEL] = df[CAPTION_COL_LABEL].apply(lambda x: text_dataset[x])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpOOhVu1yDvA"
      },
      "source": [
        "### BatchIterator class for creating data iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkV5xezMnaGi"
      },
      "source": [
        "class BatchIterator:\r\n",
        "    def __init__(self, dataframe, image_col_label, caption_col_label, batch_size, image_transformer, main_img_path, shuffle=False):\r\n",
        "        self.dataframe = dataframe\r\n",
        "        self.unique_images = np.unique(dataframe[image_col_label])\r\n",
        "        self.image_col = image_col_label\r\n",
        "        self.caption_col = caption_col_label\r\n",
        "        self.image_transformer = image_transformer\r\n",
        "        self.main_img_path = main_img_path\r\n",
        "\r\n",
        "        self.num_samples = len(self.unique_images)\r\n",
        "        self.batch_size = batch_size\r\n",
        "        self.batches_count =  self.num_samples // self.batch_size\r\n",
        "        self.shuffle = shuffle\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return self.batches_count\r\n",
        "\r\n",
        "    def __iter__(self):\r\n",
        "        indices = np.arange(self.num_samples)\r\n",
        "\r\n",
        "        if self.shuffle:\r\n",
        "            np.random.shuffle(indices)\r\n",
        "\r\n",
        "        for start in range(0, self.num_samples, self.batch_size):\r\n",
        "            end = min(start + self.batch_size, self.num_samples)\r\n",
        "\r\n",
        "            batch_indices = indices[start:end]\r\n",
        "            batch_images = []\r\n",
        "            batch_captions = []\r\n",
        "\r\n",
        "            for idx in batch_indices:\r\n",
        "                image_name = self.unique_images[idx]\r\n",
        "\r\n",
        "                image = self.__get_image_matrix(image_name)                \r\n",
        "                caption = self.__get_caption(image_name)\r\n",
        "\r\n",
        "                batch_images.append(image)\r\n",
        "                batch_captions.append(caption)\r\n",
        "\r\n",
        "            yield {\r\n",
        "                \"images\": batch_images,\r\n",
        "                \"captions\": torch.tensor(batch_captions)\r\n",
        "            }\r\n",
        "\r\n",
        "    def __get_image_matrix(self, image_name):\r\n",
        "        image = Image.open(os.path.join(self.main_img_path, image_name))\r\n",
        "        image = image.convert(\"RGB\")\r\n",
        "        return self.image_transformer(image)\r\n",
        "\r\n",
        "    def __get_caption(self, image_name):\r\n",
        "        all_captions = np.array(\r\n",
        "                        self.dataframe[self.dataframe[self.image_col] == image_name][self.caption_col]\r\n",
        "                      )\r\n",
        "        return random.choice(all_captions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI31TXLTt5nN"
      },
      "source": [
        "def split_data(dataframe, ratios):\r\n",
        "    data_len = dataframe.shape[0]\r\n",
        "\r\n",
        "    lengths = [int(data_len * ratio) for ratio in ratios]\r\n",
        "    if np.sum(lengths) != data_len:\r\n",
        "        lengths[-1] = data_len - np.sum(lengths[:-1])\r\n",
        "    \r\n",
        "    split_indices = [np.sum(lengths[:i+1]) for i in range(len(lengths))]\r\n",
        "    return np.split(df, split_indices)[:len(lengths)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vR7Jmne6x9-2"
      },
      "source": [
        "### Spliting the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeRZwaiAhdCg"
      },
      "source": [
        "train_data, valid_data, test_data = split_data(df, [0.8, 0.1, 0.1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpnhls2Qt7qw"
      },
      "source": [
        "### Creating data iterators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3I45lfuF2PMM"
      },
      "source": [
        "image_transformer = torchvision.transforms.Compose([\r\n",
        "                          torchvision.transforms.ToTensor(),\r\n",
        "                          torchvision.transforms.Resize((299, 299)),\r\n",
        "                          torchvision.transforms.Normalize([0.485, 0.456, 0.406],\r\n",
        "                                                           [0.229, 0.224, 0.225])\r\n",
        "                    ])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5PhGf-20hH0"
      },
      "source": [
        "BATCH_SIZE = 64\r\n",
        "main_img_path = \"/content/train2014\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lu1eyytEqymx"
      },
      "source": [
        "train_iterator = BatchIterator(train_data, IMAGE_COL_LABEL, CAPTION_COL_LABEL, BATCH_SIZE, image_transformer, main_img_path)\r\n",
        "valid_iterator = BatchIterator(valid_data, IMAGE_COL_LABEL, CAPTION_COL_LABEL, BATCH_SIZE, image_transformer, main_img_path)\r\n",
        "test_iterator = BatchIterator(test_data, IMAGE_COL_LABEL, CAPTION_COL_LABEL, BATCH_SIZE, image_transformer, main_img_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zu69FBZQYOzL"
      },
      "source": [
        "# Main Part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1QT-hjLzsWD"
      },
      "source": [
        "### Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fo19dru1bh1"
      },
      "source": [
        "**ENCODER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ypRxw45XJqwC"
      },
      "source": [
        "class Encoder(nn.Module):\r\n",
        "    def __init__(self, cnn_model, embedding_size, deny_train_cnn=True):\r\n",
        "        super().__init__()\r\n",
        "        self.cnn_model = cnn_model\r\n",
        "\r\n",
        "        if deny_train_cnn:\r\n",
        "            for param in self.cnn_model.parameters():\r\n",
        "                param.requires_grad = False\r\n",
        "\r\n",
        "        self.fc = nn.Linear(self.cnn_model.fc.in_features, embedding_size)\r\n",
        "\r\n",
        "    def forward(self, images):\r\n",
        "        _, features, _ = self.cnn_model(images)\r\n",
        "        features = self.fc(features)\r\n",
        "        return features "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UstfHFbjkcwt"
      },
      "source": [
        "**DECODER**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azMNeYk2jXl7"
      },
      "source": [
        "class Decoder(nn.Module):\r\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, dropout=0.3, num_layers=2, bidirectional=True, cnn_feature_size=2048):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        num_directions = 2 if bidirectional else 1\r\n",
        "        assert hidden_dim % num_directions == 0\r\n",
        "        rnn_hidden_dim = hidden_dim // num_directions\r\n",
        "\r\n",
        "        self.vocab_size = vocab_size\r\n",
        "        self.embedding_layer = nn.Embedding(vocab_size, embedding_dim)\r\n",
        "        self.rnn = nn.LSTM(embedding_dim, rnn_hidden_dim, num_layers=num_layers,\r\n",
        "                           dropout=dropout, bidirectional=bidirectional, batch_first=True)\r\n",
        "        \r\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\r\n",
        "\r\n",
        "    def forward(self, image_vectors, captions):\r\n",
        "        captions = captions[:, :-1]\r\n",
        "\r\n",
        "        embedded = self.embedding_layer(captions)\r\n",
        "        concated_data = torch.cat((image_vectors.unsqueeze(1), embedded), dim=1)\r\n",
        "        rnn_output, _ = self.rnn(concated_data)\r\n",
        "        logits = self.fc(rnn_output)\r\n",
        "        return logits "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cyeX5kVtKXd9"
      },
      "source": [
        "**SEQ-2-SEQ**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oGjQUedEgYM"
      },
      "source": [
        "class CaptionNet(nn.Module):\r\n",
        "    def __init__(self, encoder, decoder, device, teacher_forcing_ratio = 0.5):\r\n",
        "        super().__init__()\r\n",
        "\r\n",
        "        self.encoder = encoder\r\n",
        "        self.decoder = decoder\r\n",
        "        self.device = device\r\n",
        "        self.teacher_forcing_ratio = teacher_forcing_ratio\r\n",
        "\r\n",
        "    def forward(self, images, captions):\r\n",
        "        features = self.encoder(images)\r\n",
        "        outputs = self.decoder(features, captions)\r\n",
        "        return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PCpje8vpSy2"
      },
      "source": [
        "# def caption_image(self, image, vocabulary, max_length=50):\r\n",
        "#     result_caption = []\r\n",
        "\r\n",
        "#     with torch.no_grad():\r\n",
        "#         x = self.encoderCNN(image).unsqueeze(0)\r\n",
        "#         states = None\r\n",
        "\r\n",
        "#         for _ in range(max_length):\r\n",
        "#             hiddens, states = self.decoderRNN.lstm(x, states)\r\n",
        "#             output = self.decoderRNN.linear(hiddens.squeeze(0))\r\n",
        "#             predicted = output.argmax(1)\r\n",
        "#             result_caption.append(predicted.item())\r\n",
        "#             x = self.decoderRNN.embed(predicted).unsqueeze(0)\r\n",
        "\r\n",
        "#             if vocabulary.itos[predicted.item()] == \"<EOS>\":\r\n",
        "#                 break\r\n",
        "\r\n",
        "#     return [vocabulary.itos[idx] for idx in result_caption]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5eJc5H5dRt6W"
      },
      "source": [
        "    # def sample(self, inputs, states=None, max_len=20):\r\n",
        "    #     \" accepts pre-processed  image tensor (inputs) and returns predicted sentence (list of tensor ids of length max_len) \"\r\n",
        "    #     output_sentence = []\r\n",
        "    #     for i in range(max_len):\r\n",
        "    #         lstm_outputs, states = self.lstm(inputs, states)\r\n",
        "    #         lstm_outputs = lstm_outputs.squeeze(1)\r\n",
        "    #         out = self.linear(lstm_outputs)\r\n",
        "    #         last_pick = out.max(1)[1]\r\n",
        "    #         output_sentence.append(last_pick.item())\r\n",
        "    #         inputs = self.embedding_layer(last_pick).unsqueeze(1)\r\n",
        "        \r\n",
        "    #     return output_sentence"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fX1k_gJdQw9L",
        "outputId": "010e3754-c072-4963-85ca-75c4390def07"
      },
      "source": [
        "#@title Hyperparameters { run: \"auto\" }\r\n",
        "\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(\"Device: \", device)\r\n",
        "\r\n",
        "VOCAB_SIZE = len(text_dataset)\r\n",
        "EMBEDDING_DIM = 250 #@param {type:\"slider\", min:100, max:1000, step:50}\r\n",
        "HIDDEN_DIM = 256 #@param [\"64\", \"128\", \"256\", \"512\", \"1024\", \"2048\"] {type:\"raw\"}\r\n",
        "DROPOUT = 0.3 #@param {type:\"slider\", min:0, max:1, step:0.1}\r\n",
        "NUM_LAYERS = 2 #@param {type:\"slider\", min:1, max:10, step:1}\r\n",
        "BIDIRECTIONAL = True #@param {type:\"boolean\"}\r\n",
        "TEACHER_FORCE_RATIO = 0.3 #@param {type:\"slider\", min:0, max:1, step:0.1}"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xERVY_VfcP_A"
      },
      "source": [
        "Inception model loading"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170,
          "referenced_widgets": [
            "deb9c8652b8c40ec90865028ce30ea6a",
            "7d6bea0a81f44205bf8ed99cedfebdf1",
            "00b3019b8b4448cb9a6ce1f17d516835",
            "0e5e0940f61e47e1a5c1267790010fe0",
            "b20b11fe3e8949e58d6ed996447782e9",
            "18e57034e50745c58ece39c4611fa160",
            "37bf3ddae5d347eb8ab8f67d00feec1d",
            "243662727a104bf78f0b63cc437a49eb"
          ]
        },
        "id": "4ZSuJel6cKs7",
        "outputId": "ab55f03d-1f74-45d1-926c-1ce9078d8c9b"
      },
      "source": [
        "%%time\r\n",
        "from beheaded_inception3 import beheaded_inception_v3\r\n",
        "inception = beheaded_inception_v3().train(False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/models/inception.py:77: FutureWarning: The default weight initialization of inception_v3 will be changed in future releases of torchvision. If you wish to keep the old behavior (which leads to long initialization times due to scipy/scipy#11299), please set init_weights=True.\n",
            "  ' due to scipy/scipy#11299), please set init_weights=True.', FutureWarning)\n",
            "Downloading: \"https://download.pytorch.org/models/inception_v3_google-1a9a5a14.pth\" to /root/.cache/torch/hub/checkpoints/inception_v3_google-1a9a5a14.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "deb9c8652b8c40ec90865028ce30ea6a",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=108857766.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "CPU times: user 3min 15s, sys: 773 ms, total: 3min 16s\n",
            "Wall time: 3min 17s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "armn4FjEYrAP"
      },
      "source": [
        "encoder = Encoder(inception, EMBEDDING_DIM).to(device)\r\n",
        "decoder = Decoder(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, DROPOUT, NUM_LAYERS, BIDIRECTIONAL).to(device)\r\n",
        "captionNet = CaptionNet(encoder, decoder, device, TEACHER_FORCE_RATIO).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ajGSNly1N3ek",
        "outputId": "dc1ec277-7db7-45cb-8172-50b9bda5281a"
      },
      "source": [
        "def count_parameters(model):\r\n",
        "    return np.sum([param.numel() for param in model.parameters() if param.requires_grad])\r\n",
        "\r\n",
        "print(f'The model has {count_parameters(captionNet):,} trainable parameters')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 6,808,738 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RRt_2KeOg1Z7"
      },
      "source": [
        "optimizer = torch.optim.Adam(captionNet.parameters())\r\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0).to(device)\r\n",
        "# criterion = nn.CrossEntropyLoss(ignore_index=3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWpsLmnogEwD"
      },
      "source": [
        "#@title {run: \"auto\" }\r\n",
        "\r\n",
        "EPOCHS = 6 #@param {type:\"slider\", min:1, max:20, step:1}\r\n",
        "CLIP = 2 #@param {type:\"slider\", min:1, max:10, step:1}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LxE6vTE4dxXi"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4zt3rsFb9e9"
      },
      "source": [
        "def iterate_model(mode, model, iterator, optimizer=None, criterion=None, device=None, clip=None):\r\n",
        "    if mode == \"train\":\r\n",
        "        model.train()\r\n",
        "    elif mode == \"valid\":\r\n",
        "        model.eval()\r\n",
        "    else:\r\n",
        "      raise ValueError(\"Invalid mode, must be 'train' or 'valid'\")\r\n",
        "\r\n",
        "    epoch_loss = 0\r\n",
        "    epoch_perplexity = 0\r\n",
        "\r\n",
        "    history = []\r\n",
        "    for iteration, batch in tqdm(enumerate(iterator), total=len(iterator)):\r\n",
        "\r\n",
        "        images = torch.stack(batch[\"images\"]).to(device)\r\n",
        "        captions = batch[\"captions\"].to(device)\r\n",
        "\r\n",
        "        output = model(images, captions)\r\n",
        "        loss = criterion(output.view(-1, output.shape[2]), captions.view(-1))\r\n",
        "\r\n",
        "        if mode == \"train\":\r\n",
        "            optimizer.zero_grad()\r\n",
        "            loss.backward()\r\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\r\n",
        "            optimizer.step()\r\n",
        "\r\n",
        "        epoch_loss += loss.item()\r\n",
        "        epoch_perplexity += torch.exp(loss)\r\n",
        "\r\n",
        "    epoch_loss /= len(iterator)\r\n",
        "    epoch_perplexity /= len(iterator)\r\n",
        "\r\n",
        "    return epoch_loss, epoch_perplexity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABuLojdttF37"
      },
      "source": [
        "def train(epochs, model, train_iterator, valid_iterator, optimizer, criterion, device, clip, patience, save_epoch):\r\n",
        "    MIN_LOSS = np.inf\r\n",
        "    CUR_PATIENCE = 0\r\n",
        "\r\n",
        "    for epoch in range(epochs):\r\n",
        "            \r\n",
        "        train_loss, train_perplexity = iterate_model(\"train\", model, train_iterator, optimizer, criterion, device, clip)\r\n",
        "        valid_loss, valid_perplexity = iterate_model(\"valid\", model, valid_iterator, criterion=criterion, device=device)\r\n",
        "        \r\n",
        "        if valid_loss < MIN_LOSS:\r\n",
        "            MIN_LOSS = valid_loss\r\n",
        "            best_model = model.state_dict()\r\n",
        "        else:\r\n",
        "            CUR_PATIENCE += 1\r\n",
        "            if CUR_PATIENCE == patience:\r\n",
        "                CUR_PATIENCE = 0\r\n",
        "                break\r\n",
        "\r\n",
        "        if (epoch + 1) % save_epoch == 0:\r\n",
        "            torch.save(best_model, 'best-model.pt')\r\n",
        "\r\n",
        "        print(f\"Train loss: {train_loss} | Train perplexity: {train_perplexity}\")\r\n",
        "        print(f\"Valid loss: {valid_loss} | Train perplexity: {valid_perplexity}\")\r\n",
        "        print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "twzC_viAPSUJ"
      },
      "source": [
        "PATIENCE = 2\r\n",
        "SAVE_EPOCH = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBjZv2crBi7i"
      },
      "source": [
        "train(EPOCHS, captionNet, train_iterator, valid_iterator, optimizer, criterion, device, CLIP, PATIENCE, SAVE_EPOCH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PG_QWJ3OuAs4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}